{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54c63158",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from utils2 import transform_image_to_kspace, transform_kspace_to_image\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import pydicom\n",
    "from skimage.draw import line\n",
    "\n",
    "data_path = r'C:\\from SSD\\fastMRI_brain_DICOM\\100099070170\\279.dcm'\n",
    "#data_path = r'C:\\Users\\simon\\anaconda3\\envs\\drs\\drs\\Data\\fastMRI_brain_DICOM\\300008311232\\808.dcm'\n",
    "data = pydicom.dcmread(data_path)\n",
    "\n",
    "#image = data.pixel_array\n",
    "#if (np.shape(image) != (320, 320)):\n",
    "#    image = cv2.resize(image, (320, 320))\n",
    "\n",
    "#image = (image - np.mean(image)) / np.std(image)\n",
    "\n",
    "#kspace = transform_image_to_kspace(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4794367",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Simone\\anaconda3\\envs\\artefacts\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: [WinError 127] The specified procedure could not be found\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'QuantStub' from 'torch.ao.quantization' (C:\\Users\\Simone\\anaconda3\\envs\\artefacts\\lib\\site-packages\\torch\\ao\\quantization\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 16>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m logm, expm\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mndimage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m zoom\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpiq\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ssim, SSIMLoss, MultiScaleSSIMLoss, VSILoss\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m#from pytorch3d.transforms.so3 import (\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m#    so3_exponential_map,\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m#    so3_relative_angle,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# pip install torchkbnufft\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# pip install piq\u001b[39;00m\n\u001b[0;32m     28\u001b[0m image \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mload_png(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data/sample_2d.png\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mcomplex)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\artefacts\\lib\\site-packages\\piq\\__init__.py:5\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mssim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ssim, SSIMLoss\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mms_ssim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m multi_scale_ssim, MultiScaleSSIMLoss\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmsid\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MSID\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfid\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FID\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkid\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KID\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\artefacts\\lib\\site-packages\\piq\\msid.py:10\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpiq\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseFeatureMetric\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpiq\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _validate_input, _parse_version\n\u001b[0;32m     14\u001b[0m EPSILON \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-6\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\artefacts\\lib\\site-packages\\piq\\base.py:3\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpiq\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extractors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InceptionV3\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mBaseFeatureMetric\u001b[39;00m(torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Base class for all metrics, which require computation of per image features.\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124;03m     For example: FID, KID, MSID etc.\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03m     \"\"\"\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\artefacts\\lib\\site-packages\\piq\\feature_extractors\\__init__.py:1\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpiq\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extractors\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfid_inception\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InceptionV3\n\u001b[0;32m      3\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInceptionV3\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\artefacts\\lib\\site-packages\\piq\\feature_extractors\\fid_inception.py:13\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m models\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m List\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\artefacts\\lib\\site-packages\\torchvision\\__init__.py:7\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datasets\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m io\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m models\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ops\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m transforms\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\artefacts\\lib\\site-packages\\torchvision\\models\\__init__.py:18\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m feature_extraction\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m optical_flow\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m quantization\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m segmentation\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m video\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\artefacts\\lib\\site-packages\\torchvision\\models\\quantization\\__init__.py:1\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmobilenet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresnet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgooglenet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\artefacts\\lib\\site-packages\\torchvision\\models\\quantization\\mobilenet.py:1\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmobilenetv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m QuantizableMobileNetV2, mobilenet_v2, __all__ \u001b[38;5;28;01mas\u001b[39;00m mv2_all\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmobilenetv3\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m QuantizableMobileNetV3, mobilenet_v3_large, __all__ \u001b[38;5;28;01mas\u001b[39;00m mv3_all\n\u001b[0;32m      4\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m mv2_all \u001b[38;5;241m+\u001b[39m mv3_all\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\artefacts\\lib\\site-packages\\torchvision\\models\\quantization\\mobilenetv2.py:5\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tensor\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mao\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquantization\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m QuantStub, DeQuantStub\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmobilenetv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InvertedResidual, MobileNetV2, model_urls\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_internally_replaced_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_state_dict_from_url\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'QuantStub' from 'torch.ao.quantization' (C:\\Users\\Simone\\anaconda3\\envs\\artefacts\\lib\\site-packages\\torch\\ao\\quantization\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import numpy as np\n",
    "import cv2\n",
    "import nibabel as nib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchkbnufft as tkbn\n",
    "import utils\n",
    "import visualisation\n",
    "from skimage.data import shepp_logan_phantom\n",
    "from scipy.linalg import logm, expm\n",
    "from scipy.ndimage import zoom\n",
    "from piq import ssim, SSIMLoss, MultiScaleSSIMLoss, VSILoss\n",
    "\n",
    "#from pytorch3d.transforms.so3 import (\n",
    "#    so3_exponential_map,\n",
    "#    so3_relative_angle,\n",
    "#)\n",
    "\n",
    "# pip install nibabe\n",
    "# pip install torch\n",
    "# pip install torchkbnufft\n",
    "# pip install piq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f780d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Simone\\AppData\\Local\\Temp\\ipykernel_1720\\2262484877.py:1: DeprecationWarning: `np.complex` is a deprecated alias for the builtin `complex`. To silence this warning, use `complex` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.complex128` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  image = utils.load_png('./data/sample_2d.png').astype(np.complex)\n"
     ]
    }
   ],
   "source": [
    "image = utils.load_png('./data/sample_2d.png').astype(np.complex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c11c6bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Config\n",
    "debug = False\n",
    "animate = True\n",
    "dtype = torch.float32\n",
    "complex_dtype = torch.complex64\n",
    "numpoints = 6\n",
    "eps = 1e-12\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('device:', device)\n",
    "matplotlib.use(\"Agg\") if animate else None\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "mode = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28029217",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_movements(n_movements, ndims, angles_std=5.0, trans_std=10.0, dtype=torch.float32, device=None):\n",
    "    \"\"\"Sample movement affine transforms.\"\"\"\n",
    "    affines = []\n",
    "    angles = angles_std * torch.randn((n_movements+1,), dtype=dtype, device=device)\n",
    "    trans = trans_std * torch.randn((n_movements+1,2), dtype=dtype, device=device)\n",
    "    for i in range(n_movements+1):\n",
    "        ang = angles[i]\n",
    "        t = trans[i,:]\n",
    "        A = torch.eye(ndims+1).to(device)\n",
    "        R = utils.rotation_matrix_2d(ang, device=device)\n",
    "        A[:ndims,:ndims] = R.to(device)\n",
    "        A[:ndims,ndims] = t.to(device)\n",
    "        affines.append(A)\n",
    "    return affines, angles, trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f91206d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_shape = image.shape\n",
    "ndims = len(image_shape)\n",
    "sampling_rate=1.0\n",
    "kr = int(image_shape[0] * sampling_rate)\n",
    "kc = int(image_shape[1] * sampling_rate)\n",
    "grid_size = (kr, kc)\n",
    "n_movements = np.minimum(50, grid_size[0])\n",
    "\n",
    "affines, angles, trans = sample_movements(n_movements, ndims, angles_std=5.0, trans_std=10.0, device=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ddd394b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_masks(n_movements, locs, grid_size, use_torch=False):\n",
    "    \"\"\"Generate k-space masks.\"\"\"\n",
    "    masks = []\n",
    "    if n_movements > 0:\n",
    "        mask = np.zeros(grid_size)\n",
    "        mask[0:locs[0],...] = 1\n",
    "        masks.append(mask)\n",
    "        for i in range(1,n_movements):\n",
    "            mask = np.zeros(grid_size)\n",
    "            mask[locs[i-1]:locs[i],...] = 1\n",
    "            masks.append(mask)\n",
    "        mask = np.zeros(grid_size)\n",
    "        mask[locs[-1]::,...] = 1\n",
    "        masks.append(mask)\n",
    "    else:\n",
    "        masks.append(np.ones(grid_size))\n",
    "    return masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be866f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "locs, _ = torch.sort(torch.randperm(grid_size[0])[:n_movements])\n",
    "masks = gen_masks(n_movements, locs, grid_size, use_torch=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f2ffdcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_ktraj(nlines, klen, kdepth=None, use_torch=False, device=None):\n",
    "    \"\"\"Generate kx, ky\"\"\"\n",
    "    kx = torch.linspace(-np.pi, np.pi, klen)\n",
    "    ky = torch.linspace(-np.pi, np.pi, nlines)\n",
    "    kx, ky = torch.meshgrid(kx, ky)\n",
    "    kx = kx.T\n",
    "    ky = ky.T\n",
    "    \n",
    "    #kx = np.linspace(-np.pi, np.pi, klen)\n",
    "    #ky = np.linspace(-np.pi, np.pi, nlines)\n",
    "    #kx, ky = np.meshgrid(kx, ky)\n",
    "    return kx, ky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc05cbc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Simone\\anaconda3\\envs\\artefacts\\lib\\site-packages\\torch\\functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ..\\aten\\src\\ATen\\native\\TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "nlines = kr\n",
    "klen = kc\n",
    "kx, ky = gen_ktraj(nlines, klen, kdepth=None, use_torch=False, device=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e02a96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_kspace(image_shape, sampling_rate=1.0, device=None):\n",
    "    \"\"\"Construct the k-space trajectory.\"\"\"\n",
    "    ndims = len(image_shape)\n",
    "    kr = int(image_shape[0] * sampling_rate)\n",
    "    kc = int(image_shape[1] * sampling_rate)\n",
    "    grid_size = (kr, kc)\n",
    "    kx, ky = gen_ktraj(kr, kc, device=device)\n",
    "    kz = None\n",
    "    return kx, ky, kz, grid_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1544e1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "kx, ky, kz, grid_size = build_kspace(image_shape, sampling_rate=1.0, device=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "49a5a7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rotation(angles, kx, ky, kz=None, ndims=None, masks=None, dtype=torch.float32, device=None):\n",
    "    \"\"\"Apply rotation to k-space trajectory.\"\"\"\n",
    "    kx_new = torch.zeros_like(kx, device=device)\n",
    "    ky_new = torch.zeros_like(ky, device=device)\n",
    "    kz_new = None\n",
    "    for i in range(len(masks)):\n",
    "        ang = torch.deg2rad(angles[i])\n",
    "        kyi = torch.cos(ang)*ky - torch.sin(ang)*kx\n",
    "        kxi = torch.sin(ang)*ky + torch.cos(ang)*kx\n",
    "        kx_new[masks[i],...] = kxi[masks[i],...]\n",
    "        ky_new[masks[i],...] = kyi[masks[i],...]\n",
    "    return kx_new, ky_new, kz_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6f17cdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "kx_new, ky_new, kz_new = apply_rotation(angles, kx, ky, kz, ndims, masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f492e511",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_translation(ts, kdata, kx, ky, kz=None, grid_size=None, ndims=None, masks=None, dtype=torch.float32, device=None):\n",
    "    \"\"\"Apply translation as phase shift to k-space.\"\"\"\n",
    "    kdata = torch.reshape(kdata, grid_size)\n",
    "    kdata_new = torch.zeros_like(kdata, device=device)\n",
    "    for i in range(len(masks)):\n",
    "        t = ts[i]\n",
    "        kdata_i = utils.translate_opt(kdata, torch.stack((ky.flatten(), kx.flatten())), t, device=device)\n",
    "        kdata_new.real[masks[i],...] = kdata_i.real[masks[i],...]\n",
    "        kdata_new.imag[masks[i],...] = kdata_i.imag[masks[i],...]\n",
    "    kdata = kdata_new.flatten().unsqueeze(0).unsqueeze(0)\n",
    "    return kdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb36a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#kdata = apply_translation(trans, kdata, kx_new, ky_new, kz_new, grid_size, ndims, masks, dtype=dtype, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8a8fe2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_nufft(image, im_size, grid_size, numpoints, complex_dtype=torch.complex64, device=None):\n",
    "    \"\"\"Init NUFFT objects.\"\"\"\n",
    "    nufft_ob = tkbn.KbNufft(\n",
    "        im_size=im_size,\n",
    "        grid_size=grid_size,\n",
    "        numpoints=numpoints,\n",
    "        ).to(complex_dtype).to(device)\n",
    "    adjnufft_ob = tkbn.KbNufftAdjoint(\n",
    "        im_size=im_size,\n",
    "        grid_size=grid_size,\n",
    "        numpoints=numpoints,\n",
    "        ).to(torch.tensor(image)).to(device)\n",
    "    return nufft_ob, adjnufft_ob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b39c558d",
   "metadata": {},
   "outputs": [],
   "source": [
    "im_size = image.shape\n",
    "nufft_ob, adjnufft_ob = build_nufft(image, im_size, grid_size, numpoints, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "335b2232",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_nufft(nufft_ob, image, ndims, kx, ky, kz=None, dtype=torch.float32, device=device):\n",
    "    \"\"\"Nufft.\"\"\"\n",
    "    return nufft_ob(image, torch.stack((ky.flatten(), kx.flatten()))).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e8e44602",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'is_complex'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [23]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m kdata \u001b[38;5;241m=\u001b[39m \u001b[43mapply_nufft\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnufft_ob\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mndims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mky\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [22]\u001b[0m, in \u001b[0;36mapply_nufft\u001b[1;34m(nufft_ob, image, ndims, kx, ky, kz, dtype, device)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_nufft\u001b[39m(nufft_ob, image, ndims, kx, ky, kz\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39mdevice):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;124;03m\"\"\"Nufft.\"\"\"\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnufft_ob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mky\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\artefacts\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\artefacts\\lib\\site-packages\\torchkbnufft\\modules\\kbnufft.py:170\u001b[0m, in \u001b[0;36mKbNufft.forward\u001b[1;34m(self, image, omega, interp_mats, smaps, norm)\u001b[0m\n\u001b[0;32m    167\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage dtype does not match smaps dtype.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    169\u001b[0m is_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m():\n\u001b[0;32m    171\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m image\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m    172\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFor real inputs, last dimension must be size 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'is_complex'"
     ]
    }
   ],
   "source": [
    "kdata = apply_nufft(nufft_ob, image, ndims, kx, ky, kz, dtype=dtype, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "975f52f7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'numpy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [29]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mnumpy\u001b[49m\u001b[38;5;241m.\u001b[39m__version__\n",
      "\u001b[1;31mNameError\u001b[0m: name 'numpy' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47eeaf8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 192)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bc1b21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
